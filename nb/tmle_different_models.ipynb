{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Machine Learning Tutorial\n",
    "\n",
    "This notebook compares targeted maximum likelihood estimation implemented via three different methods: a logistic regression, a boosted decision tree and a superlearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to the Python path\n",
    "sys.path.append(os.path.abspath('../src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial import generate_data, produce_dag, generate_data_nonlinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf # allows R-like syntax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmle import targeting_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doubleml import DoubleMLData\n",
    "from doubleml import DoubleMLIRM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This comparison uses the same dataset as the `tmle_walkthrough.ipynb` notebook. We can change the data generating function to make it a little bit less well behaved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ATE = 0.17865750000000002\n"
     ]
    }
   ],
   "source": [
    "df = generate_data_nonlinearity(2000000, 555) # generate a huge dataset from which we can calculate the 'true' ATE\n",
    "\n",
    "true_EY1 = df['Y1'].mean()\n",
    "true_EY0 = df['Y0'].mean()\n",
    "true_ATE = true_EY1-true_EY0\n",
    "print(f'True ATE = {true_ATE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>w3</th>\n",
       "      <th>w4</th>\n",
       "      <th>A</th>\n",
       "      <th>Y</th>\n",
       "      <th>Y1</th>\n",
       "      <th>Y0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.380</td>\n",
       "      <td>2.750</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.994</td>\n",
       "      <td>3.031</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.040</td>\n",
       "      <td>4.536</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.001</td>\n",
       "      <td>3.052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007</td>\n",
       "      <td>4.160</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.777</td>\n",
       "      <td>2.746</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.777</td>\n",
       "      <td>4.488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.428</td>\n",
       "      <td>2.478</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.843</td>\n",
       "      <td>1.945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.601</td>\n",
       "      <td>1.681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       w1   w2     w3     w4    A    Y   Y1   Y0\n",
       "0     1.0  1.0  3.380  2.750  1.0  0.0  0.0  1.0\n",
       "1     0.0  0.0  3.994  3.031  1.0  1.0  1.0  1.0\n",
       "2     1.0  1.0  0.040  4.536  1.0  1.0  1.0  1.0\n",
       "3     1.0  0.0  2.001  3.052  0.0  1.0  1.0  1.0\n",
       "4     0.0  0.0  0.007  4.160  1.0  0.0  0.0  1.0\n",
       "...   ...  ...    ...    ...  ...  ...  ...  ...\n",
       "9995  0.0  1.0  1.777  2.746  1.0  1.0  1.0  1.0\n",
       "9996  1.0  1.0  0.777  4.488  1.0  1.0  1.0  1.0\n",
       "9997  1.0  1.0  1.428  2.478  1.0  1.0  1.0  0.0\n",
       "9998  0.0  1.0  1.843  1.945  0.0  1.0  0.0  1.0\n",
       "9999  1.0  1.0  1.601  1.681  0.0  1.0  0.0  1.0\n",
       "\n",
       "[10000 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now generate a much smaller dataset that we will actually use for our methods\n",
    "df = generate_data_nonlinearity(10000, 556)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a new dataset where the treatment variables are set to 1 and 0 \n",
    "\n",
    "newdata_A1 = df.copy()\n",
    "newdata_A1['A'] = 1\n",
    "\n",
    "newdata_A0 = df.copy()\n",
    "newdata_A0['A'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TMLE  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Using a binary logistic regression model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.512408\n",
      "         Iterations 6\n",
      "Initial (biased) ATE estimate = 0.18770227170831688\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.270700\n",
      "         Iterations 8\n"
     ]
    }
   ],
   "source": [
    "# first, we implement an intentionally wrong model (binary logistic regression, without the interaction terms)\n",
    "\n",
    "# this is the outcome model, fit a binary logistic regression for it \n",
    "model = smf.logit(\"Y ~ A + w1 + w2 + w3 + w4\", data=df)\n",
    "model = model.fit()\n",
    "\n",
    "# predict probabilities based on this data \n",
    "QAW = model.predict(df) # what does our model predict the outcome as (probability)\n",
    "Q1W = model.predict(newdata_A1) # what if the patient had been treated\n",
    "Q0W = model.predict(newdata_A0) # what if the patient had not been treated\n",
    "\n",
    "# initial ATE estimate: \n",
    "init_ATE_est = Q1W.mean() - Q0W.mean() # difference in outcome if every patient treated vs every patient not treated\n",
    "print(f'Initial (biased) ATE estimate = {init_ATE_est}')\n",
    "\n",
    "# this is the treatment model (propensity score) \n",
    "ps_model = smf.logit(\"A ~ w1 + w2 + w3 + w4\", data=df)\n",
    "ps_model = ps_model.fit()\n",
    "\n",
    "# use this model to calculate propensity scores \n",
    "gW = ps_model.predict(df)\n",
    "\n",
    "target_model = targeting_step(df, gW, QAW) # carries out the targeting step to optimise the b-v tradeoff for the ATE\n",
    "epsilon = target_model.params # coefficients in this targeting step\n",
    "\n",
    "# use the epsilon values to improve the treatment model\n",
    "\n",
    "# Convert Q0W and Q1W to logit scale and update them\n",
    "logit_Q0W = np.log(Q0W / (1 - Q0W))\n",
    "logit_Q1W = np.log(Q1W / (1 - Q1W))\n",
    "\n",
    "# Update logit values with epsilon adjustments\n",
    "logit_Q0W_1 = logit_Q0W + epsilon['H0W'] / (1 - gW)\n",
    "logit_Q1W_1 = logit_Q1W + epsilon['H1W'] / gW\n",
    "\n",
    "# Convert back to probabilities using inverse-logit\n",
    "Q0W_1 = expit(logit_Q0W_1)\n",
    "Q1W_1 = expit(logit_Q1W_1)\n",
    "\n",
    "# Now we can calculate an improved ATE\n",
    "EY1_tmle_1 = Q1W_1.mean()\n",
    "EY0_tmle_1 = Q0W_1.mean()\n",
    "ATE_tmle_1 = EY1_tmle_1 - EY0_tmle_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Using a ML model  \n",
    "  \n",
    "Now we use ML models for the treatment and outcome models, which should be more robust to bias and model misspecification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define predictors and outcome\n",
    "X = df[['A', 'w1', 'w2', 'w3', 'w4']]  # Covariates and treatment\n",
    "y = df['Y']  # Outcome\n",
    "\n",
    "# Initialize the Gradient Boosting model\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "gb_model.fit(X, y)\n",
    "\n",
    "QAW = gb_model.predict_proba(X)[:,1]\n",
    "Q1W = gb_model.predict_proba(newdata_A1[['A', 'w1', 'w2', 'w3', 'w4']])[:,1]\n",
    "Q0W = gb_model.predict_proba(newdata_A0[['A', 'w1', 'w2', 'w3', 'w4']])[:,1]\n",
    "\n",
    "ml_ATE_est_no_target = Q1W.mean() - Q0W.mean() # initial estimate of ATE using ML, but no targeting\n",
    "\n",
    "# Define predictors and outcome\n",
    "X = df[['w1', 'w2', 'w3', 'w4']]  # Covariates and treatment\n",
    "y = df['A']  # Outcome\n",
    "\n",
    "# Initialize the Gradient Boosting model\n",
    "ps_model_ml = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "ps_model_ml.fit(X, y)\n",
    "\n",
    "propensity_estimates_ml = ps_model_ml.predict_proba(X)[:,1]\n",
    "\n",
    "target_model = targeting_step(df, propensity_estimates_ml, QAW) # carries out the targeting step to optimise the b-v tradeoff for the ATE\n",
    "epsilon = target_model.params # coefficients in this targeting step\n",
    "\n",
    "# use the epsilon values to improve the treatment model\n",
    "\n",
    "# Convert Q0W and Q1W to logit scale and update them\n",
    "logit_Q0W = np.log(Q0W / (1 - Q0W))\n",
    "logit_Q1W = np.log(Q1W / (1 - Q1W))\n",
    "\n",
    "# Update logit values with epsilon adjustments\n",
    "logit_Q0W_1 = logit_Q0W + epsilon['H0W'] / (1 - propensity_estimates_ml)\n",
    "logit_Q1W_1 = logit_Q1W + epsilon['H1W'] / propensity_estimates_ml\n",
    "\n",
    "# Convert back to probabilities using inverse-logit\n",
    "Q0W_1 = expit(logit_Q0W_1)\n",
    "Q1W_1 = expit(logit_Q1W_1)\n",
    "\n",
    "# Now we can calculate an improved ATE\n",
    "EY1_tmle_2 = Q1W_1.mean()\n",
    "EY0_tmle_2 = Q0W_1.mean()\n",
    "ATE_tmle_2 = EY1_tmle_2 - EY0_tmle_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Using stacked classifiers (superlearner)  \n",
    "  \n",
    "This approach uses an ensemble of ML methods, as recommended by the developers of the TMLE `R` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predictors and outcome\n",
    "X = df[['A', 'w1', 'w2', 'w3', 'w4']]  # Covariates and treatment\n",
    "y = df['Y']  # Outcome\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "    ('gradient_boosting', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)),\n",
    "    ('random_forest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('svm', SVC(probability=True, kernel='linear', random_state=42))\n",
    "]\n",
    "\n",
    "# Define meta-model\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Create the stacking classifier\n",
    "stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)\n",
    "\n",
    "# Fit the stacking model\n",
    "stacking_model.fit(X, y)\n",
    "\n",
    "# Predict probabilities\n",
    "QA = stacking_model.predict_proba(X)[:,1]\n",
    "Q1 = stacking_model.predict_proba(newdata_A1[['A', 'w1', 'w2', 'w3', 'w4']])[:,1]\n",
    "Q0 = stacking_model.predict_proba(newdata_A0[['A', 'w1', 'w2', 'w3', 'w4']])[:,1]\n",
    "\n",
    "sl_ATE_est_no_target = Q1.mean() - Q0.mean() # initial estimate of ATE using superlearner, but no targeting\n",
    "\n",
    "# Define predictors and outcome\n",
    "X = df[['w1', 'w2', 'w3', 'w4']]  # Covariates and treatment\n",
    "y = df['A']  # Outcome\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "    ('gradient_boosting', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)),\n",
    "    ('random_forest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('svm', SVC(probability=True, kernel='linear', random_state=42))\n",
    "]\n",
    "\n",
    "# Define meta-model\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Create the stacking classifier\n",
    "stacking_model_ps = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)\n",
    "\n",
    "# Fit the stacking model\n",
    "stacking_model_ps.fit(X, y)\n",
    "\n",
    "propensity_estimates_sl = stacking_model_ps.predict_proba(X)[:,1]\n",
    "\n",
    "target_model = targeting_step(df, propensity_estimates_sl, QA) # carries out the targeting step to optimise the b-v tradeoff for the ATE\n",
    "epsilon = target_model.params # coefficients in this targeting step\n",
    "\n",
    "# use the epsilon values to improve the treatment model\n",
    "\n",
    "# Convert Q0W and Q1W to logit scale and update them\n",
    "logit_Q0 = np.log(Q0 / (1 - Q0))\n",
    "logit_Q1 = np.log(Q1 / (1 - Q1))\n",
    "\n",
    "# Update logit values with epsilon adjustments\n",
    "logit_Q0_1 = logit_Q0 + epsilon['H0W'] / (1 - propensity_estimates_sl)\n",
    "logit_Q1_1 = logit_Q1 + epsilon['H1W'] / propensity_estimates_sl\n",
    "\n",
    "# Convert back to probabilities using inverse-logit\n",
    "Q0_1 = expit(logit_Q0_1)\n",
    "Q1_1 = expit(logit_Q1_1)\n",
    "\n",
    "# Now we can calculate an improved ATE\n",
    "EY1_tmle_3 = Q1_1.mean()\n",
    "EY0_tmle_3 = Q0_1.mean()\n",
    "ATE_tmle_3 = EY1_tmle_3 - EY0_tmle_3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarise Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_summary = pd.DataFrame({\n",
    "    'True ATE': [true_ATE, 100*abs(true_ATE-true_ATE)/true_ATE], \n",
    "    'ATE (L.R)': [ATE_tmle_1, 100*abs(ATE_tmle_1-true_ATE)/true_ATE], \n",
    "    'ATE (ML)': [ATE_tmle_2, 100*abs(ATE_tmle_2-true_ATE)/true_ATE], \n",
    "    'ATE (Superlearning)': [ATE_tmle_3, 100*abs(ATE_tmle_3-true_ATE)/true_ATE] \n",
    "}, index=['Value', 'Difference from ATE (%)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         True ATE  ATE (L.R)  ATE (ML)  ATE (Superlearning)\n",
      "Value                    0.178658   0.182663  0.191273             0.176185\n",
      "Difference from ATE (%)  0.000000   2.241931  7.061223             1.383896\n"
     ]
    }
   ],
   "source": [
    "print(result_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
